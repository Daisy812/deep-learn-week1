# 暑期DL学习week1-xliang

## 什么是Pytorch？

- Pytorch是使用Python编写的用于构建深度学习模型的框架，提供了非常强大的函数，而不必程序猿自己重复造轮子。

## Pytorch的基础练习

​		自从寒假结束几乎就没有再碰过Pytorch，这次的基础练习帮我重新温习了一下相关基础操作，如张量的运算需要数据的统一，包括维度和数据类型的一致。

## 螺旋数据分类

​		之前相当长的一段时间习惯用别人的代码，这次自己重新从基础开始搭建模型，对nn模型的创建过程和基本结构又有了更深刻的认识。

​		通过对螺旋数据的可视化可知线性分类器并不能很好的对数据进行分类，结果损失值很高而精确度很低，单纯的线性分类无法将螺旋样本从平面中分割开来。

![1](D:\作业\dl\week1\1.JPG)

而加入Relu激活函数之后的分类效果相当不错，因为Relu激活函数为模型增加了非线性因素，使得模型对数据的分类不仅仅是在平面上的线性分类。

![2](D:\作业\dl\week1\2.JPG)

其中Relu激活函数公式表示：
$$
f(x)=max(0,x)
$$
如果网络模型不加入非线性因素，那么无论神经网络的深度是多少，依然是在做线性运算，故无法解决非线性问题。

## 问题总结

1. AlexNet有哪些特点？为什么可以比LeNet取得更好的性能？

   ​		AlexNet相比于LeNet在卷积、池化后加入了Relu激活函数，代替sigmod函数对非线性输入有更好的优化效果，且Relu函数的计算成本也更低。

   ​		AlexNet相比于LeNet使用了dropout ，让激活值可以以一定的概率停止工作，在防止过拟合上有相当重要的意义。

   ​		以上的特点使得AlexNet相比于LeNet有着更好的性能，同时也掀开了深度学习发展史上重要的一页。

2. 激活函数有哪些作用？

   ​		激活函数是用来加入非线性因素的，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。

3. 梯度消失现象是什么？

   ​		虽然激活函数的作用趋于一致，但是使用不同的激活函数的效果也是有区别的。包括sigmod激活函数和双曲正切激活函数都存在着梯度消失，在使用上诉所说的两个激活函数的情况下，如果神经元的输出接近于0或者1时，其梯度趋近于0，反向传播时该神经元的权重便不会更新，从而也会影响与该神经元相连的其他神经元，造成模型的权重几乎停止更新。

4. 神经网络是更宽好还是更深好？

   ​		神经网络的宽度和深度并不是一个对立的关系，合适的神经网络应该在宽度和深度上都兼具，增加网络的深度可以使得对输入的学习更加深入，学习数据更深层次的特征。比如对花的分类，如果原网络对花的分类是根据花瓣的性状进行学习，但是适当增加网络深度，网络则可能学习到花蕊的性状，对花的分类更加精准和鲁棒。神经网络的宽度具体指每层神经的通道数，保证每一层可以学习到丰富的特征。

   ​		但是不管深度还是宽度都是需要一个适合平衡点，否则模型的性能要么受限无法发挥到最大效果，要么计算成本过高，费时费力。

5. 为什么要使用Softmax?

   ​		softmax是归一化指数函数，目的是将原本处于负无穷到正无穷区间上的预测结果归一化到(0,1)区间上且所有预测结果相加为1。

6. SGD 和 Adam 哪个更有效？

​		不同的数据集和任务目标两者的效果可能各有优劣。

- SGD是随机梯度下降，计算成本更低，但是有可能在局部动荡，需要设置合适的学习率调整。

- Adam是自适应学习率方法，可以对参数的更新状态做出相应的调整，相应的计算成本更高。

​		但是这个世界也不一定是非黑即白的，两者也并不对立，可以采取两者结合。比如先使用SGD到局部最优解无法跳出时再使用Adam，合作共赢，两者的价值才能相得益彰。

